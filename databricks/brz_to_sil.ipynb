{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee2942e-f115-4599-9c92-5bc2edbf941f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"rebuild\", \"False\", [\"True\", \"False\"])\n",
    "rebuild = True if dbutils.widgets.get(\"rebuild\") == \"True\" else \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c542aaef-eaee-483f-b4ff-b88cf8afcc76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_scope_name = \"fmdp-secrets\"\n",
    "client_id = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=secret_scope_name, key=\"tenant-id\")\n",
    "alpha_vantage_api_key = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-alpha-vantage-api-key\")\n",
    "\n",
    "storage_account_name = \"fmdpstg2\"\n",
    "bronze_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/bronze\"\n",
    "silver_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/silver\"\n",
    "gold_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb69b538-9326-4bd6-97b4-0f7c146a8e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "  f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\": \"OAuth\",\n",
    "  f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\": client_id,\n",
    "  f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\": client_secret,\n",
    "  f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "}\n",
    "\n",
    "for k, v in configs.items(): spark.conf.set(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efecb6e8-841b-465a-a1af-c21d3a2e2e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_date, col, lit, to_date, row_number, from_json, schema_of_json, explode, map_keys, min as min_func, max as max_func, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, LongType, BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f916912-d6d0-4cb6-88d7-3f89f780abe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def av_time_series_daily_brz_to_sil(symbols=None, start_date=None, end_date=None, rebuild=False):\n",
    "    \"\"\"\n",
    "    Transform TIME_SERIES_DAILY data from bronze to silver layer.\n",
    "    \n",
    "    Args:\n",
    "        symbols (list, optional): List of stock symbols to process. If None, process all symbols.\n",
    "        start_date (str, optional): Start date for processing in YYYY-MM-DD format\n",
    "        end_date (str, optional): End date for processing in YYYY-MM-DD format\n",
    "        rebuild (bool): Whether to rebuild the entire silver layer or perform incremental update\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if transformation successful, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Define table paths\n",
    "        bronze_table = f\"{bronze_path}/brz_av_time_series_daily\"\n",
    "        silver_table = f\"{silver_path}/sil_av_time_series_daily\"\n",
    "        \n",
    "        # Create silver directory if it doesn't exist\n",
    "        silver_dir = silver_path\n",
    "        dbutils.fs.mkdirs(silver_dir)\n",
    "        \n",
    "        # Start time for performance tracking\n",
    "        start_time = datetime.now()\n",
    "        print(f\"Starting silver transformation at {start_time}\")\n",
    "        \n",
    "        # Define explicit schema for silver layer\n",
    "        silver_schema = StructType([\n",
    "            StructField(\"symbol\", StringType(), False),\n",
    "            StructField(\"date\", DateType(), False),\n",
    "            StructField(\"open\", DoubleType(), True),\n",
    "            StructField(\"high\", DoubleType(), True),\n",
    "            StructField(\"low\", DoubleType(), True),\n",
    "            StructField(\"close\", DoubleType(), True),\n",
    "            StructField(\"volume\", LongType(), True),\n",
    "            StructField(\"source_batch_id\", StringType(), True),\n",
    "            StructField(\"source_load_type\", StringType(), True),\n",
    "            StructField(\"is_valid\", BooleanType(), False),\n",
    "            StructField(\"processing_date\", DateType(), False),\n",
    "            StructField(\"processing_timestamp\", TimestampType(), False)\n",
    "        ])\n",
    "        \n",
    "        # Read from bronze layer\n",
    "        print(\"Reading from bronze layer...\")\n",
    "        bronze_df = spark.read.format(\"delta\").load(bronze_table)\n",
    "        \n",
    "        # Filter by symbols if provided\n",
    "        if symbols:\n",
    "            bronze_df = bronze_df.filter(col(\"symbol\").isin(symbols))\n",
    "        \n",
    "        # Filter by ingestion date if provided\n",
    "        if start_date:\n",
    "            bronze_df = bronze_df.filter(col(\"ingestion_date\") >= start_date)\n",
    "        if end_date:\n",
    "            bronze_df = bronze_df.filter(col(\"ingestion_date\") <= end_date)\n",
    "        \n",
    "        if not rebuild:\n",
    "            # For incremental updates, only process the latest batch for each symbol\n",
    "            print(\"Performing incremental update - selecting latest batches...\")\n",
    "            window_spec = Window.partitionBy(\"symbol\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "            bronze_df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                                .filter(col(\"row_num\") == 1) \\\n",
    "                                .drop(\"row_num\")\n",
    "        else:\n",
    "            # For full rebuild, check if we have any initial load records\n",
    "            has_full = bronze_df.filter(col(\"load_type\") == \"full\").count() > 0\n",
    "            \n",
    "            if has_full:\n",
    "                print(\"Rebuild mode: Found full load records, prioritizing those...\")\n",
    "                # For each symbol, prioritize initial load if available, otherwise use latest batches\n",
    "                window_spec = Window.partitionBy(\"symbol\").orderBy(\n",
    "                    # First prioritize initial loads, then by timestamp\n",
    "                    when(col(\"load_type\") == \"full\", 0).otherwise(1),\n",
    "                    col(\"ingestion_timestamp\").desc()\n",
    "                )\n",
    "                bronze_df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                                    .filter(col(\"row_num\") == 1) \\\n",
    "                                    .drop(\"row_num\")\n",
    "            else:\n",
    "                print(\"Rebuild mode: No initial load records found, using all available data...\")\n",
    "                # Use the latest batch for each symbol since no initial load is available\n",
    "                window_spec = Window.partitionBy(\"symbol\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "                bronze_df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                                    .filter(col(\"row_num\") == 1) \\\n",
    "                                    .drop(\"row_num\")\n",
    "        \n",
    "        # Count symbols being processed\n",
    "        symbol_count = bronze_df.select(\"symbol\").distinct().count()\n",
    "        if symbol_count == 0:\n",
    "            print(\"No data to process. Exiting.\")\n",
    "            return True\n",
    "            \n",
    "        print(f\"Processing data for {symbol_count} symbols\")\n",
    "        \n",
    "        # Create an empty DataFrame with our desired schema\n",
    "        empty_rdd = spark.sparkContext.emptyRDD()\n",
    "        silver_df = spark.createDataFrame(empty_rdd, silver_schema)\n",
    "        \n",
    "        # Process each symbol individually to handle the complex nested structure\n",
    "        for symbol_row in bronze_df.collect():\n",
    "            symbol = symbol_row.symbol\n",
    "            batch_id = symbol_row.batch_id\n",
    "            load_type = symbol_row.load_type\n",
    "            raw_data = json.loads(symbol_row.raw_data)\n",
    "            \n",
    "            print(f\"Processing symbol: {symbol} (load type: {load_type})\")\n",
    "            \n",
    "            # Extract time series data\n",
    "            time_series_data = raw_data.get(\"Time Series (Daily)\", {})\n",
    "            \n",
    "            # Create rows for each date\n",
    "            rows = []\n",
    "            for date_str, daily_data in time_series_data.items():\n",
    "                try:\n",
    "                    # Parse values with error handling\n",
    "                    open_price = float(daily_data.get(\"1. open\", 0)) if daily_data.get(\"1. open\") else None\n",
    "                    high_price = float(daily_data.get(\"2. high\", 0)) if daily_data.get(\"2. high\") else None\n",
    "                    low_price = float(daily_data.get(\"3. low\", 0)) if daily_data.get(\"3. low\") else None\n",
    "                    close_price = float(daily_data.get(\"4. close\", 0)) if daily_data.get(\"4. close\") else None\n",
    "                    volume = int(daily_data.get(\"5. volume\", 0)) if daily_data.get(\"5. volume\") else None\n",
    "                    \n",
    "                    # Check if data is valid\n",
    "                    is_valid = (\n",
    "                        open_price is not None and \n",
    "                        high_price is not None and \n",
    "                        low_price is not None and \n",
    "                        close_price is not None and \n",
    "                        volume is not None\n",
    "                    )\n",
    "                    \n",
    "                    # Create a row\n",
    "                    rows.append((\n",
    "                        symbol,                         # symbol\n",
    "                        datetime.strptime(date_str, \"%Y-%m-%d\").date(),  # date\n",
    "                        open_price,                     # open\n",
    "                        high_price,                     # high\n",
    "                        low_price,                      # low\n",
    "                        close_price,                    # close\n",
    "                        volume,                         # volume\n",
    "                        batch_id,                       # source_batch_id\n",
    "                        load_type,                      # source_load_type\n",
    "                        is_valid,                       # is_valid\n",
    "                        datetime.now().date(),          # processing_date\n",
    "                        datetime.now()                  # processing_timestamp\n",
    "                    ))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {symbol} for date {date_str}: {str(e)}\")\n",
    "            \n",
    "            # Create a DataFrame from the rows for this symbol\n",
    "            if rows:\n",
    "                symbol_df = spark.createDataFrame(rows, silver_schema)\n",
    "                \n",
    "                # Union with the main DataFrame\n",
    "                silver_df = silver_df.union(symbol_df)\n",
    "        \n",
    "        # Count the records before writing\n",
    "        total_records = silver_df.count()\n",
    "        print(f\"Processed {total_records} records for {symbol_count} symbols\")\n",
    "        \n",
    "        if total_records == 0:\n",
    "            print(\"No records to write. Exiting.\")\n",
    "            return True\n",
    "            \n",
    "        # Write to silver layer\n",
    "        print(\"Writing to silver layer...\")\n",
    "        \n",
    "        # Check if silver table exists and choose appropriate write strategy\n",
    "        silver_table_exists = True\n",
    "        try:\n",
    "            # Try to read the silver table to check if it exists\n",
    "            spark.read.format(\"delta\").load(silver_table).limit(1).count()\n",
    "        except:\n",
    "            silver_table_exists = False\n",
    "        \n",
    "        if rebuild or not silver_table_exists:\n",
    "            # For rebuilds or new tables, overwrite completely\n",
    "            print(\"Using overwrite mode for silver layer...\")\n",
    "            silver_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .partitionBy(\"symbol\") \\\n",
    "                .save(silver_table)\n",
    "        else:\n",
    "            # For incremental updates, use merge operation\n",
    "            print(\"Using merge operation for incremental update...\")\n",
    "            # Create DeltaTable instance\n",
    "            delta_table = DeltaTable.forPath(spark, silver_table)\n",
    "            \n",
    "            # Perform MERGE operation\n",
    "            delta_table.alias(\"target\") \\\n",
    "                .merge(\n",
    "                    silver_df.alias(\"source\"),\n",
    "                    \"target.symbol = source.symbol AND target.date = source.date\"\n",
    "                ) \\\n",
    "                .whenMatchedUpdateAll() \\\n",
    "                .whenNotMatchedInsertAll() \\\n",
    "                .execute()\n",
    "        \n",
    "        # Calculate statistics for reporting\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Read back the silver table after write to get accurate counts\n",
    "        updated_silver_df = spark.read.format(\"delta\").load(silver_table)\n",
    "        \n",
    "        # Get record counts\n",
    "        total_silver_records = updated_silver_df.count()\n",
    "        valid_records = updated_silver_df.filter(col(\"is_valid\") == True).count()\n",
    "        invalid_records = total_silver_records - valid_records\n",
    "        \n",
    "        # Calculate date range correctly using spark functions\n",
    "        date_min = updated_silver_df.agg(min_func(\"date\")).collect()[0][0]\n",
    "        date_max = updated_silver_df.agg(max_func(\"date\")).collect()[0][0]\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n--- Silver Transformation Summary ---\")\n",
    "        print(f\"Operation: {'Rebuild' if rebuild else 'Incremental update'}\")\n",
    "        print(f\"Completed at: {end_time}\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"Symbols processed: {symbol_count}\")\n",
    "        print(f\"Records processed in this run: {total_records}\")\n",
    "        print(f\"Total records in silver layer: {total_silver_records}\")\n",
    "        print(f\"Valid records: {valid_records}\")\n",
    "        print(f\"Invalid records: {invalid_records}\")\n",
    "        print(f\"Date range: {date_min} to {date_max}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in silver transformation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e0d83e-0da0-46d0-a00b-da3a42d7f61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of symbols to process (can be None to process all symbols in bronze)\n",
    "symbols = [\"AAPL\", \"MSFT\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\", \"GOOGL\", \"QQQ\", \"SPY\"]\n",
    "\n",
    "# Optional date filters (can be None to process all dates)\n",
    "start_date = None  # Format: \"2023-01-01\"\n",
    "end_date = None    # Format: \"2023-12-31\"\n",
    "\n",
    "# Choose the operation mode\n",
    "# - For first run after initial data load, set rebuild=True\n",
    "# - For daily updates after that, set rebuild=False\n",
    "\n",
    "print(f\"Running {'SILVER REBUILD' if rebuild else 'SILVER UPDATE'} at {datetime.now()}\")\n",
    "\n",
    "# Add timestamp for logging\n",
    "run_start_time = datetime.now()\n",
    "print(f\"Starting silver transformation at {run_start_time}\")\n",
    "\n",
    "try:\n",
    "    # Call the transformation function\n",
    "    success = av_time_series_daily_brz_to_sil(\n",
    "        symbols=symbols,        # List of symbols to process (or None for all)\n",
    "        start_date=start_date,  # Optional date filter\n",
    "        end_date=end_date,      # Optional date filter\n",
    "        rebuild=rebuild    # Rebuild mode flag\n",
    "    )\n",
    "    \n",
    "    # Summarize results\n",
    "    run_end_time = datetime.now()\n",
    "    duration = (run_end_time - run_start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n--- Transformation Summary ---\")\n",
    "    print(f\"Operation: {'Rebuild' if rebuild else 'Incremental update'}\")\n",
    "    print(f\"Status: {'Successful' if success else 'Failed'}\")\n",
    "    print(f\"Run completed at: {run_end_time}\")\n",
    "    print(f\"Total duration: {duration:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in transformation process: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c8a45f-f278-40a3-b9be-24efdfab7289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "brz_to_sil",
   "widgets": {
    "rebuild": {
     "currentValue": "False",
     "nuid": "7d11e968-376d-4469-89e5-b5f04034f969",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "",
      "name": "rebuild",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "",
      "name": "rebuild",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
