{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d2c5f3-0f65-40c9-b41b-155047ad62fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"run_mode\", \"incremental\", [\"incremental\", \"full\"])\n",
    "run_mode = dbutils.widgets.get(\"run_mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67256f32-5d71-4688-9b51-9107c9cb8e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_scope_name = \"fmdp-secrets\"\n",
    "client_id = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=secret_scope_name, key=\"tenant-id\")\n",
    "alpha_vantage_api_key = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-alpha-vantage-api-key\")\n",
    "\n",
    "storage_account_name = \"fmdpstg2\"\n",
    "bronze_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/bronze\"\n",
    "silver_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/silver\"\n",
    "gold_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "557cfb29-9db0-48da-a69e-69eb3e819cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "  f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\": \"OAuth\",\n",
    "  f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\": client_id,\n",
    "  f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\": client_secret,\n",
    "  f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "}\n",
    "\n",
    "for k, v in configs.items(): spark.conf.set(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5f5e01-e9b0-41c9-8eab-8803fba3aa41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_date, col, lit, to_date\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba42bc21-fc7f-4214-a0e5-b955fd1a30a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def av_time_series_daily_src_to_brz(symbol, outputsize=\"compact\", full_load=False):\n",
    "    \"\"\"\n",
    "    Incrementally ingest TIME_SERIES_DAILY data from Alpha Vantage API to bronze layer\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Stock ticker symbol\n",
    "        outputsize (str): 'compact' (last 100 data points) or 'full' (20+ years of data)\n",
    "        full_load (bool): Whether this is an full load (full history) or daily update\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if ingestion successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Override outputsize if full_load is specified\n",
    "    if full_load:\n",
    "        outputsize = \"full\"  # Always use full for full load\n",
    "    \n",
    "    function = \"TIME_SERIES_DAILY\"\n",
    "    batch_id = f\"{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    ingestion_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Build URL with appropriate parameters\n",
    "    url = f\"https://www.alphavantage.co/query?function={function}&symbol={symbol}&outputsize={outputsize}&apikey={alpha_vantage_api_key}\"\n",
    "    \n",
    "    try:\n",
    "        # Make API request with retry logic\n",
    "        for attempt in range(3):  # 3 retries\n",
    "            try:\n",
    "                response = requests.get(url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                raw_data = response.json()\n",
    "                break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == 2:  # Last attempt\n",
    "                    raise\n",
    "                print(f\"Retrying request for {symbol} (attempt {attempt+1}/3)\")\n",
    "                time.sleep(5)  # Wait before retry\n",
    "        \n",
    "        # Check for errors or rate limiting\n",
    "        if \"Error Message\" in raw_data:\n",
    "            print(f\"API Error for {symbol}: {raw_data['Error Message']}\")\n",
    "            return False\n",
    "            \n",
    "        if \"Note\" in raw_data and \"API call frequency\" in raw_data[\"Note\"]:\n",
    "            print(f\"Rate limited for {symbol}: {raw_data['Note']}\")\n",
    "            time.sleep(15)  # Sleep and could retry\n",
    "            return False\n",
    "            \n",
    "        # Verify data structure\n",
    "        if \"Meta Data\" not in raw_data or \"Time Series (Daily)\" not in raw_data:\n",
    "            print(f\"Unexpected data structure for {symbol}\")\n",
    "            return False\n",
    "            \n",
    "        # Get last refreshed date from metadata\n",
    "        last_refreshed = raw_data[\"Meta Data\"].get(\"3. Last Refreshed\", ingestion_date)\n",
    "        \n",
    "        # Generate response hash for change detection\n",
    "        response_hash = hashlib.md5(json.dumps(raw_data, sort_keys=True).encode()).hexdigest()\n",
    "        \n",
    "        # Bronze table path\n",
    "        bronze_table = f\"{bronze_path}/brz_av_time_series_daily\"\n",
    "        \n",
    "        # Get data range information for logging\n",
    "        time_series = raw_data.get(\"Time Series (Daily)\", {})\n",
    "        data_dates = list(time_series.keys())\n",
    "        data_dates.sort()  # Sort dates for proper range reporting\n",
    "        \n",
    "        earliest_date = data_dates[0] if data_dates else \"N/A\"\n",
    "        latest_date = data_dates[-1] if data_dates else \"N/A\"\n",
    "        date_count = len(data_dates)\n",
    "        \n",
    "        print(f\"Retrieved {date_count} days of data for {symbol}: {earliest_date} to {latest_date}\")\n",
    "        \n",
    "        # Check if we already have this exact data\n",
    "        try:\n",
    "            existing_df = spark.read.format(\"delta\").load(bronze_table)\n",
    "            existing_batch_rcount = existing_df.filter(\n",
    "                (col(\"symbol\") == symbol) & \n",
    "                (col(\"response_hash\") == response_hash) &\n",
    "                (col(\"last_refreshed\") == last_refreshed)\n",
    "            ).count()\n",
    "            \n",
    "            if existing_batch_rcount > 0:\n",
    "                print(f\"Skipping {symbol}: Data unchanged since last ingestion\")\n",
    "                return True\n",
    "        except:\n",
    "            # Table doesn't exist yet - first run\n",
    "            pass\n",
    "            \n",
    "        # Create DataFrame with enhanced metadata\n",
    "        df_brz = spark.createDataFrame([\n",
    "            (\n",
    "                symbol,                      # Symbol\n",
    "                batch_id,                    # Batch ID\n",
    "                outputsize,                  # Data volume requested\n",
    "                last_refreshed,              # Date of last data point\n",
    "                response_hash,               # Hash for change detection\n",
    "                raw_data[\"Meta Data\"].get(\"1. Information\", \"\"),  # API info\n",
    "                raw_data[\"Meta Data\"].get(\"2. Symbol\", symbol),   # Symbol from API\n",
    "                json.dumps(raw_data),        # Full raw JSON\n",
    "                earliest_date,               # Earliest date in the dataset\n",
    "                latest_date,                 # Latest date in the dataset\n",
    "                date_count,                  # Number of dates in the dataset\n",
    "                \"full\" if full_load else \"incremental\"  # Load type\n",
    "            )\n",
    "        ], [\"symbol\", \"batch_id\", \"outputsize\", \"last_refreshed\", \"response_hash\", \n",
    "            \"information\", \"api_symbol\", \"raw_data\", \"earliest_date\", \"latest_date\",\n",
    "            \"date_count\", \"load_type\"])\n",
    "        \n",
    "        # Add ingestion metadata\n",
    "        df_brz = df_brz.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        df_brz = df_brz.withColumn(\"ingestion_date\", to_date(lit(ingestion_date)))\n",
    "        \n",
    "        # Write to Delta table\n",
    "        df_brz.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .partitionBy(\"symbol\", \"ingestion_date\") \\\n",
    "            .save(bronze_table)\n",
    "            \n",
    "        msg = \"full historical\" if full_load else \"incremental\"\n",
    "        print(f\"Successfully ingested {symbol} TIME_SERIES_DAILY {msg} data (batch {batch_id})\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error ingesting {symbol}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ef6242-ebb0-4b10-945e-49466b4b42c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "symbols = [\"AAPL\", \"MSFT\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\", \"GOOGL\", \"QQQ\", \"SPY\"]\n",
    "\n",
    "# Get current run configuration - determine if this is full load or daily update\n",
    "# You could set this based on a parameter, schedule, or presence of data\n",
    "\n",
    "# Add timestamp for logging\n",
    "run_start_time = datetime.now()\n",
    "print(f\"Starting data ingestion at {run_start_time} in {run_mode} mode\")\n",
    "\n",
    "successful_symbols = []\n",
    "failed_symbols = []\n",
    "\n",
    "# Process each symbol\n",
    "for i, symbol in enumerate(symbols):\n",
    "    print(f\"Processing {i+1}/{len(symbols)}: {symbol}\")\n",
    "    \n",
    "    try:\n",
    "        # Call the source-to-bronze function with appropriate parameters\n",
    "        if run_mode == \"full\":\n",
    "            # For full load, get full historical data\n",
    "            success = av_time_series_daily_src_to_brz(symbol, full_load=True)\n",
    "        else:\n",
    "            # For daily updates, just get the latest data\n",
    "            success = av_time_series_daily_src_to_brz(symbol, full_load=False)\n",
    "            \n",
    "        if success:\n",
    "            successful_symbols.append(symbol)\n",
    "        else:\n",
    "            failed_symbols.append(symbol)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol}: {str(e)}\")\n",
    "        failed_symbols.append(symbol)\n",
    "    \n",
    "    # Manage API rate limits\n",
    "    if run_mode == \"full\":\n",
    "        # For full load (getting 20+ years of data), need longer pauses\n",
    "        if (i + 1) % 5 == 0 and i < len(symbols) - 1:\n",
    "            print(f\"API limit reached, sleeping for 60 seconds... ({i+1}/{len(symbols)} completed)\")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            # Small delay between individual requests\n",
    "            time.sleep(10)  # Slightly longer delay for large data requests\n",
    "    else:\n",
    "        # For incremental updates (getting 100 days), can use shorter pauses\n",
    "        if (i + 1) % 5 == 0 and i < len(symbols) - 1:\n",
    "            print(f\"API limit reached, sleeping for 60 seconds... ({i+1}/{len(symbols)} completed)\")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            # Small delay between individual requests\n",
    "            time.sleep(5)\n",
    "\n",
    "# Summarize results\n",
    "run_end_time = datetime.now()\n",
    "duration = (run_end_time - run_start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n--- Ingestion Summary ---\")\n",
    "print(f\"Run mode: {run_mode}\")\n",
    "print(f\"Run completed at: {run_end_time}\")\n",
    "print(f\"Total duration: {duration:.2f} seconds\")\n",
    "print(f\"Successful: {len(successful_symbols)}/{len(symbols)} ({', '.join(successful_symbols) if successful_symbols else 'None'})\")\n",
    "print(f\"Failed: {len(failed_symbols)}/{len(symbols)} ({', '.join(failed_symbols) if failed_symbols else 'None'})\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "src_to_brz",
   "widgets": {
    "run_mode": {
     "currentValue": "full",
     "nuid": "d7657b1b-edaa-4e2a-83c4-5dd0004061d0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "incremental",
      "label": null,
      "name": "run_mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "incremental",
        "full"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "incremental",
      "label": null,
      "name": "run_mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "incremental",
        "full"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
