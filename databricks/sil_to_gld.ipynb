{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93713a6e-04d4-4490-8f3b-4ab00c513b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_scope_name = \"fmdp-secrets\"\n",
    "client_id = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=secret_scope_name, key=\"tenant-id\")\n",
    "alpha_vantage_api_key = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-alpha-vantage-api-key\")\n",
    "\n",
    "storage_account_name = \"fmdpstg2\"\n",
    "bronze_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/bronze\"\n",
    "silver_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/silver\"\n",
    "gold_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933973b4-bd03-4a9e-82f6-007a89f77670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "  f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\": \"OAuth\",\n",
    "  f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\": client_id,\n",
    "  f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\": client_secret,\n",
    "  f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "}\n",
    "\n",
    "for k, v in configs.items(): spark.conf.set(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a5cbd3-17b8-46cf-93de-f60a9aa32867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_date, col, lit, to_date, lag, avg, stddev, first, lit, min, max, datediff, when, abs, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49511a5f-3ce6-4545-b3f7-523c7e67f1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def av_time_series_daily_sil_to_gld():\n",
    "    \"\"\"\n",
    "    Transform silver stock data to gold layer with financial metrics\n",
    "    \n",
    "    Args:\n",
    "        silver_path (str): Path to silver data\n",
    "        gold_path (str): Path to write gold data\n",
    "    \"\"\"\n",
    "    print(f\"Starting silver to gold transformation...\")\n",
    "    \n",
    "    # Load silver data\n",
    "    silver_table = f\"{silver_path}/sil_av_time_series_daily\"\n",
    "    df_sil = spark.read.format(\"delta\").load(silver_table)\n",
    "    \n",
    "    # Get the count of records by symbol for reporting\n",
    "    symbol_counts = df_sil.groupBy(\"symbol\").count().collect()\n",
    "    symbol_count = len(symbol_counts)\n",
    "    total_records = df_sil.count()\n",
    "    print(f\"Processing {total_records} records for {symbol_count} symbols\")\n",
    "    \n",
    "    # Define window for time-based calculations\n",
    "    symbol_window = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "    \n",
    "    # Daily return (%)\n",
    "    df = df_sil.withColumn(\"daily_return_pct\", \n",
    "        when(lag(\"close\").over(symbol_window).isNull(), None)\n",
    "        .otherwise(((col(\"close\") - lag(\"close\").over(symbol_window)) / lag(\"close\").over(symbol_window)) * 100)\n",
    "    )\n",
    "    \n",
    "    # 5-day and 20-day moving averages\n",
    "    df = df.withColumn(\"ma_5\", avg(\"close\").over(symbol_window.rowsBetween(-4, 0)))\n",
    "    df = df.withColumn(\"ma_20\", avg(\"close\").over(symbol_window.rowsBetween(-19, 0)))\n",
    "    \n",
    "    # Additional moving averages for longer trends\n",
    "    df = df.withColumn(\"ma_50\", avg(\"close\").over(symbol_window.rowsBetween(-49, 0)))\n",
    "    df = df.withColumn(\"ma_200\", avg(\"close\").over(symbol_window.rowsBetween(-199, 0)))\n",
    "    \n",
    "    # 5-day rolling volatility (stddev of daily return)\n",
    "    df = df.withColumn(\"volatility_5d\", stddev(\"daily_return_pct\").over(symbol_window.rowsBetween(-4, 0)))\n",
    "    \n",
    "    # 20-day Volatility for longer-term view\n",
    "    df = df.withColumn(\"volatility_20d\", stddev(\"daily_return_pct\").over(symbol_window.rowsBetween(-19, 0)))\n",
    "    \n",
    "    # Normalized close price (base = 100)\n",
    "    first_close = first(\"close\").over(Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0))\n",
    "    df = df.withColumn(\"normalized_close\", (col(\"close\") / first_close) * 100)\n",
    "    \n",
    "    # Trading signals based on technical indicators\n",
    "    df = df.withColumn(\"signal_ma_crossover\", \n",
    "        when(\n",
    "            (col(\"ma_5\") > col(\"ma_20\")) & (lag(\"ma_5\").over(symbol_window) <= lag(\"ma_20\").over(symbol_window)), \n",
    "            \"BUY\"\n",
    "        ).when(\n",
    "            (col(\"ma_5\") < col(\"ma_20\")) & (lag(\"ma_5\").over(symbol_window) >= lag(\"ma_20\").over(symbol_window)), \n",
    "            \"SELL\"\n",
    "        ).otherwise(\"HOLD\")\n",
    "    )\n",
    "    \n",
    "    # Calculate Relative Strength (compared to market - assuming SPY is the market)\n",
    "    # First, calculate daily performance for each symbol\n",
    "    spy_data = df.filter(col(\"symbol\") == \"SPY\").select(\"date\", \"daily_return_pct\").withColumnRenamed(\"daily_return_pct\", \"spy_return\")\n",
    "    \n",
    "    # Join with SPY data to compare performance\n",
    "    df = df.join(spy_data, \"date\", \"left\")\n",
    "    \n",
    "    # Calculate relative strength\n",
    "    df = df.withColumn(\"relative_strength\", col(\"daily_return_pct\") - col(\"spy_return\"))\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df = df.withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    \n",
    "    # Write to gold zone\n",
    "    gold_table = f\"{gold_path}/gld_av_time_series_daily\"\n",
    "    df.write \\\n",
    "      .format(\"delta\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .option(\"overwriteSchema\", \"true\") \\\n",
    "      .save(gold_table)\n",
    "    \n",
    "    # Get statistics for reporting\n",
    "    gold_df = spark.read.format(\"delta\").load(gold_table)\n",
    "    gold_count = gold_df.count()\n",
    "    \n",
    "    print(f\"Silver to gold transformation complete.\")\n",
    "    print(f\"Wrote {gold_count} records to gold layer.\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e5d867-041c-4c90-8f5c-d8e5a2833562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "av_time_series_daily_sil_to_gld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13005764-e6ae-4286-8788-79278a2c996a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load(f'{gold_path}/gld_av_time_series_daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f7fdcc-01a0-4345-a960-f00b2e74819a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53a5e001-3304-4a74-a3ff-e430e68d6a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sil_to_gld",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
