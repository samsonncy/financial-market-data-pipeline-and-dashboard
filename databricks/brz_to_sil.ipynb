{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c542aaef-eaee-483f-b4ff-b88cf8afcc76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret_scope_name = \"fmdp-secrets\"\n",
    "client_id = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-id\")\n",
    "client_secret = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-databricks-sp-client-secret\")\n",
    "tenant_id = dbutils.secrets.get(scope=secret_scope_name, key=\"tenant-id\")\n",
    "alpha_vantage_api_key = dbutils.secrets.get(scope=secret_scope_name, key=\"fmdp-alpha-vantage-api-key\")\n",
    "\n",
    "storage_account_name = \"fmdpstg2\"\n",
    "bronze_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/bronze\"\n",
    "silver_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/silver\"\n",
    "gold_path = f\"abfss://financial-data@{storage_account_name}.dfs.core.windows.net/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb69b538-9326-4bd6-97b4-0f7c146a8e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "  f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\": \"OAuth\",\n",
    "  f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\": client_id,\n",
    "  f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\": client_secret,\n",
    "  f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "}\n",
    "\n",
    "for k, v in configs.items(): spark.conf.set(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efecb6e8-841b-465a-a1af-c21d3a2e2e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_date, col, lit, to_date, row_number, from_json, schema_of_json, explode, map_keys\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, LongType, BooleanType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce56a24-47fd-4e7f-96e1-72ed7e06f528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def av_time_series_daily_brz_to_sil(symbols=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Transform TIME_SERIES_DAILY data from bronze to silver layer.\n",
    "    \n",
    "    Args:\n",
    "        symbols (list, optional): List of stock symbols to process. If None, process all symbols.\n",
    "        start_date (str, optional): Start date for processing in YYYY-MM-DD format\n",
    "        end_date (str, optional): End date for processing in YYYY-MM-DD format\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if transformation successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import necessary functions\n",
    "        from pyspark.sql.functions import col, min as min_func, max as max_func\n",
    "        from datetime import datetime\n",
    "        import json\n",
    "        \n",
    "        # Define table paths\n",
    "        bronze_table = f\"{bronze_path}/brz_av_time_series_daily\"\n",
    "        silver_table = f\"{silver_path}/sil_av_time_series_daily\"\n",
    "        \n",
    "        # Create silver directory if it doesn't exist\n",
    "        silver_dir = silver_path\n",
    "        dbutils.fs.mkdirs(silver_dir)\n",
    "        \n",
    "        # Start time for performance tracking\n",
    "        start_time = datetime.now()\n",
    "        print(f\"Starting silver transformation at {start_time}\")\n",
    "        \n",
    "        # Define explicit schema for silver layer\n",
    "        silver_schema = StructType([\n",
    "            StructField(\"symbol\", StringType(), False),\n",
    "            StructField(\"date\", DateType(), False),\n",
    "            StructField(\"open\", DoubleType(), True),\n",
    "            StructField(\"high\", DoubleType(), True),\n",
    "            StructField(\"low\", DoubleType(), True),\n",
    "            StructField(\"close\", DoubleType(), True),\n",
    "            StructField(\"volume\", LongType(), True),\n",
    "            StructField(\"source_batch_id\", StringType(), True),\n",
    "            StructField(\"is_valid\", BooleanType(), False),\n",
    "            StructField(\"processing_date\", DateType(), False),\n",
    "            StructField(\"processing_timestamp\", TimestampType(), False)\n",
    "        ])\n",
    "        \n",
    "        # Read from bronze layer\n",
    "        print(\"Reading from bronze layer...\")\n",
    "        bronze_df = spark.read.format(\"delta\").load(bronze_table)\n",
    "        \n",
    "        # Filter by symbols if provided\n",
    "        if symbols:\n",
    "            bronze_df = bronze_df.filter(col(\"symbol\").isin(symbols))\n",
    "        \n",
    "        # Filter by ingestion date if provided\n",
    "        if start_date:\n",
    "            bronze_df = bronze_df.filter(col(\"ingestion_date\") >= start_date)\n",
    "        if end_date:\n",
    "            bronze_df = bronze_df.filter(col(\"ingestion_date\") <= end_date)\n",
    "            \n",
    "        # Get latest data for each symbol to avoid processing old snapshots\n",
    "        print(\"Identifying latest data for each symbol...\")\n",
    "        window_spec = Window.partitionBy(\"symbol\").orderBy(col(\"ingestion_timestamp\").desc())\n",
    "        latest_df = bronze_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                            .filter(col(\"row_num\") == 1) \\\n",
    "                            .drop(\"row_num\")\n",
    "        \n",
    "        # Count symbols being processed\n",
    "        symbol_count = latest_df.select(\"symbol\").distinct().count()\n",
    "        if symbol_count == 0:\n",
    "            print(\"No data to process. Exiting.\")\n",
    "            return True\n",
    "            \n",
    "        print(f\"Processing latest data for {symbol_count} symbols\")\n",
    "        \n",
    "        # Create an empty DataFrame with our desired schema\n",
    "        empty_rdd = spark.sparkContext.emptyRDD()\n",
    "        silver_df = spark.createDataFrame(empty_rdd, silver_schema)\n",
    "        \n",
    "        # Process each symbol individually to handle the complex nested structure\n",
    "        for symbol_row in latest_df.collect():\n",
    "            symbol = symbol_row.symbol\n",
    "            batch_id = symbol_row.batch_id\n",
    "            raw_data = json.loads(symbol_row.raw_data)\n",
    "            \n",
    "            print(f\"Processing symbol: {symbol}\")\n",
    "            \n",
    "            # Extract time series data\n",
    "            time_series_data = raw_data.get(\"Time Series (Daily)\", {})\n",
    "            \n",
    "            # Create rows for each date\n",
    "            rows = []\n",
    "            for date_str, daily_data in time_series_data.items():\n",
    "                try:\n",
    "                    # Parse values with error handling\n",
    "                    open_price = float(daily_data.get(\"1. open\", 0)) if daily_data.get(\"1. open\") else None\n",
    "                    high_price = float(daily_data.get(\"2. high\", 0)) if daily_data.get(\"2. high\") else None\n",
    "                    low_price = float(daily_data.get(\"3. low\", 0)) if daily_data.get(\"3. low\") else None\n",
    "                    close_price = float(daily_data.get(\"4. close\", 0)) if daily_data.get(\"4. close\") else None\n",
    "                    volume = int(daily_data.get(\"5. volume\", 0)) if daily_data.get(\"5. volume\") else None\n",
    "                    \n",
    "                    # Check if data is valid\n",
    "                    is_valid = (open_price is not None and \n",
    "                               high_price is not None and \n",
    "                               low_price is not None and \n",
    "                               close_price is not None and \n",
    "                               volume is not None)\n",
    "                    \n",
    "                    # Create a row\n",
    "                    rows.append((\n",
    "                        symbol,                         # symbol\n",
    "                        datetime.strptime(date_str, \"%Y-%m-%d\").date(),  # date\n",
    "                        open_price,                     # open\n",
    "                        high_price,                     # high\n",
    "                        low_price,                      # low\n",
    "                        close_price,                    # close\n",
    "                        volume,                         # volume\n",
    "                        batch_id,                       # source_batch_id\n",
    "                        is_valid,                       # is_valid\n",
    "                        datetime.now().date(),          # processing_date\n",
    "                        datetime.now()                  # processing_timestamp\n",
    "                    ))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {symbol} for date {date_str}: {str(e)}\")\n",
    "            \n",
    "            # Create a DataFrame from the rows for this symbol\n",
    "            if rows:\n",
    "                symbol_df = spark.createDataFrame(rows, silver_schema)\n",
    "                \n",
    "                # Union with the main DataFrame\n",
    "                silver_df = silver_df.union(symbol_df)\n",
    "        \n",
    "        # Count the records before writing\n",
    "        total_records = silver_df.count()\n",
    "        print(f\"Processed {total_records} records for {symbol_count} symbols\")\n",
    "        \n",
    "        if total_records == 0:\n",
    "            print(\"No records to write. Exiting.\")\n",
    "            return True\n",
    "            \n",
    "        # Write to silver layer\n",
    "        print(\"Writing to silver layer...\")\n",
    "        \n",
    "        # Just use overwrite mode for simplicity since this is the initial load\n",
    "        silver_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .partitionBy(\"symbol\") \\\n",
    "            .save(silver_table)\n",
    "            \n",
    "        print(\"Successfully wrote data to silver layer\")\n",
    "        \n",
    "        # Calculate statistics for reporting\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Get record counts\n",
    "        valid_records = silver_df.filter(col(\"is_valid\") == True).count()\n",
    "        invalid_records = total_records - valid_records\n",
    "        \n",
    "        # Calculate date range correctly using spark functions\n",
    "        date_min = silver_df.agg(min_func(\"date\")).collect()[0][0]\n",
    "        date_max = silver_df.agg(max_func(\"date\")).collect()[0][0]\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n--- Silver Transformation Summary ---\")\n",
    "        print(f\"Completed at: {end_time}\")\n",
    "        print(f\"Duration: {duration:.2f} seconds\")\n",
    "        print(f\"Symbols processed: {symbol_count}\")\n",
    "        print(f\"Total records: {total_records}\")\n",
    "        print(f\"Valid records: {valid_records}\")\n",
    "        print(f\"Invalid records: {invalid_records}\")\n",
    "        print(f\"Date range: {date_min} to {date_max}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in silver transformation: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2904f8-c15d-4bb1-aa2f-5234ca90cf2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_all_bronze_to_silver():\n",
    "    \"\"\"\n",
    "    Process all data from the bronze layer to the silver layer for TIME_SERIES_DAILY.\n",
    "    Handles all available symbols and dates without filtering.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import necessary functions\n",
    "        from pyspark.sql.functions import col, min as min_func, max as max_func\n",
    "        \n",
    "        print(\"Starting full bronze to silver transformation for all historical data...\")\n",
    "        \n",
    "        # Get list of all symbols in the bronze layer\n",
    "        bronze_table = f\"{bronze_path}/brz_av_time_series_daily\"\n",
    "        \n",
    "        # Read the bronze table to get all unique symbols\n",
    "        bronze_df = spark.read.format(\"delta\").load(bronze_table)\n",
    "        all_symbols = [row.symbol for row in bronze_df.select(\"symbol\").distinct().collect()]\n",
    "        \n",
    "        symbol_count = len(all_symbols)\n",
    "        print(f\"Found {symbol_count} symbols in bronze layer: {', '.join(all_symbols)}\")\n",
    "        \n",
    "        # Call the transformation function with no filters to process everything\n",
    "        success = av_time_series_daily_brz_to_sil()\n",
    "        \n",
    "        if success:\n",
    "            # Verify the data in silver layer\n",
    "            silver_table = f\"{silver_path}/sil_av_time_series_daily\"\n",
    "            silver_df = spark.read.format(\"delta\").load(silver_table)\n",
    "            \n",
    "            # Get statistics using proper column references\n",
    "            total_records = silver_df.count()\n",
    "            symbols_processed = silver_df.select(\"symbol\").distinct().count()\n",
    "            min_date = silver_df.agg(min_func(\"date\")).collect()[0][0]\n",
    "            max_date = silver_df.agg(max_func(\"date\")).collect()[0][0]\n",
    "            \n",
    "            print(\"\\n--- Silver Layer Data Summary ---\")\n",
    "            print(f\"Total symbols: {symbols_processed}/{symbol_count}\")\n",
    "            print(f\"Total records: {total_records}\")\n",
    "            print(f\"Date range: {min_date} to {max_date}\")\n",
    "            \n",
    "            # Print record counts by symbol\n",
    "            print(\"\\nRecords by symbol:\")\n",
    "            symbol_counts = silver_df.groupBy(\"symbol\").count().orderBy(\"symbol\")\n",
    "            for row in symbol_counts.collect():\n",
    "                print(f\"  {row.symbol}: {row.count} records\")\n",
    "                \n",
    "            return True\n",
    "        else:\n",
    "            print(\"Transformation failed. Check logs for details.\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in full bronze to silver processing: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f15b59-faed-464d-b24b-d6271e044e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_all_bronze_to_silver()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "brz_to_sil",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
